{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f95c5acf8c3e89e0b0e482c04e0b8abf",
          "grade": false,
          "grade_id": "cell-e98be1092b48b377",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "uZkAeFx5Won9"
      },
      "source": [
        "# Sprint Challenge\n",
        "## *Data Science Sprint 13*\n",
        "\n",
        "After a sprint of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
        "\n",
        "The real dataset is massive (almost 8 gigs uncompressed). The data is sampled for you to something more manageable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge.\n",
        "\n",
        "## Challenge Objectives\n",
        "Successfully complete all these objectives to earn full credit.\n",
        "\n",
        "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
        "\n",
        "There are 8 total possible points in this sprint challenge.\n",
        "\n",
        "\n",
        "There are more details on each objective further down in the notebook.*\n",
        "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
        "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
        "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on Yelp rating\n",
        "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
        "\n",
        "____\n",
        "\n",
        "# Before you submit your notebook you must first\n",
        "\n",
        "1) Restart your notebook's Kernel\n",
        "\n",
        "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
        "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu.\n",
        "\n",
        "3) **Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section).**\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a0fb09b9e122fc2f2a2baae91a22a818",
          "grade": false,
          "grade_id": "cell-e6c3d2173420a581",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "zqBBFNkyWooE"
      },
      "source": [
        "### Part 0: Import Necessary Packages\n",
        "For this section, you will need to import:\n",
        "- `spacy`\n",
        "- `Pandas`\n",
        "- `Seaborn`\n",
        "- `Matplotlib`\n",
        "- `NearestNeighbors`\n",
        "- `Pipeline`\n",
        "- `TfidfVectorizer`\n",
        "- `KneighborsClassifier`\n",
        "- `GridSearchCV`\n",
        "- `corpora`\n",
        "- `LdaModel`\n",
        "- `gensim`\n",
        "- `re`\n",
        "\n",
        "> **Note: This assignment is optimized to work with these specific packages. You can use import different packages, but note that this may affect how CodeGrade works, and may cause CodeGrade to fail.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "kyF4GrNBN5uf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis==3.3.1"
      ],
      "metadata": {
        "id": "8Zvlc3H5N0YP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models"
      ],
      "metadata": {
        "id": "AUF6U0f5o1nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "-m pip install --upgrade pip"
      ],
      "metadata": {
        "id": "VSc3W6L4ovPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8d3d0719eecd5609256125d84cc4218a",
          "grade": false,
          "grade_id": "cell-b29df5c5bfb8c0d8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "dF2vmVyfWooG"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "import spacy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "eQz9XV62TES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6b8ad4a1ac317df7b82aced26eee406f",
          "grade": true,
          "grade_id": "cell-be1ef923d085ceb5",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "_cfmGoWxWooI"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "assert pd.__package__ == 'pandas'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "11b700564f5d76c1ec246d8fece821c1",
          "grade": false,
          "grade_id": "cell-c94bee05bece8c59",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "qIFhWaEBWooI"
      },
      "source": [
        "\n",
        "\n",
        "### Part 0: Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "150e28699f961709cb59be5e0f8ddbe0",
          "grade": false,
          "grade_id": "cell-395851cd95d17235",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "w1-RRgDKWooJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb4fac6-6ffd-4538-b039-ef7d2ac5b047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load reviews from URL\n",
        "data_url = 'https://raw.githubusercontent.com/bloominstituteoftechnology/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
        "\n",
        "# Import data into a DataFrame named df\n",
        "# YOUR CODE HERE\n",
        "df = pd.read_json(data_url, lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "356579363f311da83f4ef7abaf3c9212",
          "grade": true,
          "grade_id": "cell-cb5006475e42b8f9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "R5R6sPSdWooJ"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
        "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "aedd47e33e28a74846b51e236deef316",
          "grade": false,
          "grade_id": "cell-27dc6b438d2f2722",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "DrrBob4sWooN"
      },
      "source": [
        "## Part 1: Tokenize Function\n",
        "<a id=\"#p1\"></a>\n",
        "\n",
        "Complete the function `tokenize`. Your function should\n",
        "- Accept one document at a time\n",
        "- Return a list of tokens\n",
        "\n",
        "You are free to use any method you have learned this week.\n",
        "\n",
        "**TO PASS CODEGRADE RUNTIME:**\n",
        "- Do not run your tokenize function more than one time in your notebook! It is not needed until Part 4!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NN_pE43SWooO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce91f0f9-36f6-4a14-dd4c-d2417270ecd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
        "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
        "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4837ed2a1cc13057ba40203859d46ff6",
          "grade": false,
          "grade_id": "cell-3d570d5a1cd6cb64",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "W50zhv_0WooP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4c4b51-e7f5-40ca-b276-1eb6dad04b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "def tokenize(doc):\n",
        "  \"\"\"\n",
        "  This function accepts a document and returns a list of tokens in the form of lemmas.\n",
        "  It leverages the en_core_web_sm model in spaCy for tokenization\n",
        "  \"\"\"\n",
        "  # Parse the document with spaCy\n",
        "  doc = nlp(doc)\n",
        "\n",
        "  # Create a list of tokens\n",
        "  tokens = [token.lemma_ for token in doc if (token.is_stop != True) and (token.is_punct != True)]\n",
        "\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2181ca9d36070260b1f75dcfd9e58965",
          "grade": true,
          "grade_id": "cell-02da164f6fbe730a",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ztA-Bk13WooQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b8a5e2-4552-4440-b5e5-423c31963766"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "'''Testing'''\n",
        "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "sample_doc = df.sample(n=1)[\"text\"].iloc[0]\n",
        "print(tokenize(sample_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0zsvMV4Qu_j",
        "outputId": "b9d4847e-57ec-4a2f-ab03-6fcd6416aadc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['move', 'March', '2013', 'leave', 'December', '2013', 'show', 'display', 'apartment', 'room', 'decent', 'tell', 'similar', 'design', 'style', 'eventually', 'come', 'realize', 'word', 'display', 'exactly', 'show', 'actual', 'apartment', 'move', 'day', '20', 'year', 'date', 'especially', 'bathroom', 'upset', 'neighbor', 'meet', 'move', 'nearly', 'time', 'type', 'studio', 'nearly', 'mirror', 'condition', 'display', 'apartment', 'water', 'white', 'try', 'inquire', 'office', 'tell', 'oh', 's', 'mesa', 'water', 'like', 'wait', 'clear', ' ', 'wait', 'nearly', '30', 'min', 'shower', 'hot', 'constant', 'roach', 'matter', 'clean', 'spray', 'speak', 'exterminator', 'tell', 'come', '2', 'week', 'come', 'twice', 'entire', 'span', '11', 'month', 'People', 'constantly', 'park', 'parking', 'spot', 'pay', 'extra', 'loud', 'neighbor', 'laughable', 'security', 'patrol', 'apartment', 'bad', 'month', 'stay', 'management', 'constantly', 'rude', 'speak', 'tenant', 'child', 'sarcasm', 'completely', 'unprofessional', 'time', 'issue', 'approach', 'manager', 'deal', 'sarcasm', 'attitude', 'thing', 'foot', 'stand', 'ground', 'recommend', 'apartment', 'bad', 'enemy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d4137c3ea2fa84821d1dbf1b28dde6dd",
          "grade": false,
          "grade_id": "cell-ef13337bc7694c52",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "e-Eu5KKDWooQ"
      },
      "source": [
        "## Part 2: Vector Representation\n",
        "<a id=\"#p2\"></a>\n",
        "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
        "    * Name that doc-term matrix `dtm`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fefca7db0abb1474d316d6aa24e032f8",
          "grade": false,
          "grade_id": "cell-0e96491cb529202c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "PxNqu_-fWooR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ce2510-b00d-4cea-8028-774ef91d93cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 18s, sys: 1.27 s, total: 5min 19s\n",
            "Wall time: 6min\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Initialize the Tfidfvectorizer\n",
        "vect = TfidfVectorizer(tokenizer=tokenize, min_df=0.25, max_df=0.98, ngram_range=(1,2))\n",
        "\n",
        "# Create a document-term matrix\n",
        "dtm = vect.fit_transform(df['text'])\n",
        "\n",
        "# Convert the dtm to a DataFrame for easier visualization\n",
        "dtm = pd.DataFrame(dtm.todense(), columns=vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "86048b7ea6cb011227aefa5a8f7a9e65",
          "grade": false,
          "grade_id": "cell-33c058ea193687c3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5J5BbNgEWooS"
      },
      "source": [
        "\n",
        "2. Write a fake review. Assign the text of the review to an object called `fake_review`.\n",
        "3. Query the fake review for the 10 most similar reviews, print the text of the reviews.\n",
        "    - Given the size of the dataset, use `NearestNeighbors` model for this. Name the model `nn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f6aa466983420c836879d744ffa6c9a8",
          "grade": false,
          "grade_id": "cell-3d5bc610a8ec6b24",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kamza3MxWooS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80936933-1090-4c22-d42d-a44daa88b241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "219    I had an appointment at 10 a.m. for a simple o...\n",
            "66     Fit for a bride helped me clean up my mothers ...\n",
            "231    Closed   There's other guns there or you can w...\n",
            "24     Their kids menu has Kraft Mac n Cheese. The pr...\n",
            "109    The main course was just ok but that may have ...\n",
            "254    Absolutely fantastic if you are looking for so...\n",
            "229    Last Saturday I could not get a hold of my eld...\n",
            "259    Very reasonable happy hour prices ($3,4,5) for...\n",
            "203    This was the beat buffet I've been to in my en...\n",
            "26     Just about every high end retailer has a store...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Create and fit a NearestNeighbors model named \"nn\"\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Initialize the NearestNeighbors model\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
        "\n",
        "# Fit the model on the dtm\n",
        "nn.fit(dtm)\n",
        "\n",
        "# Write a fake review\n",
        "fake_review = 'This product is amazing!! I love it and use it everyday'\n",
        "\n",
        "# Transform the fake review into a dtm\n",
        "fake_review_dtm = vect.transform([fake_review])\n",
        "\n",
        "# Use the model to find the most similar reviews\n",
        "distances, indices = nn.kneighbors(np.asarray(fake_review_dtm.todense()))\n",
        "\n",
        "# Print the most similar reviews\n",
        "for index in indices:\n",
        "  print(df['text'].iloc[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d270ed23df3c7d3c6cf08ab174ccaf9e",
          "grade": true,
          "grade_id": "cell-c43704dcff67e99b",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "eq-QjgRMWooU"
      },
      "outputs": [],
      "source": [
        "'''Testing.'''\n",
        "assert nn.__module__ == 'sklearn.neighbors._unsupervised', ' nn is not a NearestNeighbors instance.'\n",
        "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "33e150190aa62764e07f1f6c66bb9393",
          "grade": true,
          "grade_id": "cell-203092260fb65165",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8vOrorlqWooV"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "assert isinstance(fake_review, str), \"Did you write a review in the correct data type?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FZdChi-WooW"
      },
      "source": [
        "## Part 3: Classification\n",
        "<a id=\"#p3\"></a>\n",
        "Your goal in this section will be to predict `stars` from the review dataset.\n",
        "\n",
        "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
        "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels).\n",
        "    - Use that pipeline to predict a star rating for your fake review from Part 2.\n",
        "\n",
        "\n",
        "\n",
        "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`.\n",
        "    - Include 2 possible values for each parameter\n",
        "        - **Keep the values for each parameter low. Extreme values will compromise runtime**\n",
        "    - **Use `n_jobs` = 1**\n",
        "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
        "    \n",
        "    \n",
        "3. Train the entire pipeline with a GridSearch\n",
        "    - Name your GridSearch object as `gs`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "Trbwz80oUOVY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "jupyter": {
          "outputs_hidden": true
        },
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b3492e82185541e6a463f46b16baff94",
          "grade": false,
          "grade_id": "cell-e2beb0252d274bba",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "kSiM2mquWooW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c9647f-9c7b-491f-b9d0-67c6dae4dc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vect', TfidfVectorizer(tokenizer=tokenize)),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Create parameter dictionary\n",
        "parameters = {\n",
        "    'vect__max_df': (0.75, 1.0),\n",
        "    'clf__max_depth': (5, 10)\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "gs = GridSearchCV(pipeline, parameters, cv=5, n_jobs=1, verbose=1)\n",
        "\n",
        "# Fit the GridSeachCV object to the data\n",
        "gs.fit(df['text'], df['stars'])\n",
        "\n",
        "# Predict a star rating for the fake review\n",
        "fake_review_prediction = gs. predict(fake_review)\n",
        "print('Predicted star rating for the fake review:', fake_review_prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ada8e7da1ec21f54451752e97b8cec3e",
          "grade": true,
          "grade_id": "cell-d07134c6fe5d056e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "281T-QQdWooW"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "prediction = gs.predict([\"This is your prediction statement.\"])[0]\n",
        "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2990aa9aa4e9c3cf665cee4392cdab92",
          "grade": false,
          "grade_id": "cell-00b8cbd0b1b4ece5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8uSg6-48WooW"
      },
      "source": [
        "## Part 4: Topic Modeling\n",
        "\n",
        "Let's find out what those yelp reviews are saying! :D\n",
        "\n",
        "1. Estimate a LDA topic model of the review text\n",
        "    - Set num_topics to `5`\n",
        "    - Name your LDA model `lda`\n",
        "2. Create 1-2 visualizations of the results\n",
        "    - You can use the most important 3 words of a topic in relevant visualizations.\n",
        "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
        "\n",
        "When you instantiate your LDA model, it should look like this:\n",
        "\n",
        "```python\n",
        "lda = LdaModel(corpus=corpus,\n",
        "               id2word=id2word,\n",
        "               random_state=723812,\n",
        "               num_topics = num_topics,\n",
        "               passes=1\n",
        "              )\n",
        "\n",
        "```\n",
        "\n",
        "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9b07079124654b07cce6d10dae1912b6",
          "grade": false,
          "grade_id": "cell-9eee6fe0eeebb9a3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BAiH4YCnWooX"
      },
      "source": [
        "## Note about  pyLDAvis\n",
        "\n",
        "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
        "\n",
        "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.**\n",
        "\n",
        "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "58830f560044227aa07c22d463e1596c",
          "grade": false,
          "grade_id": "cell-ec7b71ad284832d4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "I8gYc8CGWooX"
      },
      "source": [
        "### 1. Estimate a LDA topic model of the review text\n",
        "\n",
        "* Use the `tokenize` function you created earlier to create tokens.\n",
        "* Create an `id2word` object.\n",
        "> Hint: Use `corpora.Dictionary`\n",
        "* Create a `corpus` object.\n",
        "> Hint: Use `id2word.doc2bow`\n",
        "* Instantiate an `lda` model.\n",
        "\n",
        ">> Remember to read the LDA docs for more information on the various class attributes and methods available to you in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fb50f495592df233d97bd4199b958404",
          "grade": false,
          "grade_id": "cell-66331a185ff52f15",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "t0ajFfVoWooY"
      },
      "outputs": [],
      "source": [
        "# Tokenize the reviews\n",
        "df['tokens'] = df['text'].apply(tokenize)\n",
        "\n",
        "# Create an id2word object\n",
        "id2word = corpora.Dictionary(df['tokens'])\n",
        "\n",
        "# Create a corpus object\n",
        "corpus = [id2word.doc2bow(text) for text in df['tokens']]\n",
        "\n",
        "# Define the number of topics\n",
        "num_topics = 5\n",
        "\n",
        "# Instantiate an LDA model\n",
        "lda = LdaModel(corpus=corpus,\n",
        "               id2word=id2word,\n",
        "               random_state=723812,\n",
        "               num_topics = num_topics,\n",
        "               passes=1\n",
        "              )\n",
        "\n",
        "# Print the topics and teir most important words\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZd-0TqVWooY"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "073be746ce974f75f29c2c92f35af430",
          "grade": true,
          "grade_id": "cell-5a3c181311134fa9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "CndXVV15WooY"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "\n",
        "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy5QyrvVWooY"
      },
      "source": [
        "#### 2. Create 2 visualizations of the results:\n",
        "1. Create a visualization using pyLDAvis. Run the cell, then comment out your code before submission, leaving the visualization in the cell.\n",
        "\n",
        "2. Create a visualization using the matplotlib library and utilizing the subplots function. Assign this visualization to a variable called `visual_plot`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNM6C6zXWooZ"
      },
      "outputs": [],
      "source": [
        "# Cell for pyLDAvis visualization\n",
        "# YOUR CODE HERE\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim_models.prepare(lda, corpus, id2word)\n",
        "vis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2cb1397c6a59aa5751d77bad34994f29",
          "grade": false,
          "grade_id": "cell-9b043e992fbd218c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "eiY8ae4mWooZ"
      },
      "outputs": [],
      "source": [
        "# Cell for matplotlib visualzation\n",
        "# YOUR CODE HERE\n",
        "fig, ax = plt.subplots(figzise=(10,10))\n",
        "\n",
        "for i in range(num_topics):\n",
        "  x = [term[0] for term in lda.show_topic(i, 10)]\n",
        "  y = [term[1] for term in lda.show_topic(i, 10)]\n",
        "\n",
        "  ax.scatter(x, y, label=f'Topic {i+1}')\n",
        "  for j, txt in enumerate(x):\n",
        "    ax.annotate(txt, (x[j], y[j]))\n",
        "\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "visual_plot = fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "97e1c557c7e019c69cc2714b055fb767",
          "grade": true,
          "grade_id": "cell-f5fa579a25122b47",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BGze3hBsWooZ"
      },
      "outputs": [],
      "source": [
        "# Visible Testing\n",
        "assert visual_plot.__module__ == 'matplotlib.axes._subplots', \"You must create and assign to visual_plot a visualization\"\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "u4-s1-nlp"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "toc-autonumbering": false,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}