* What is the advantage(s) of using a classification pipeline in NLP?
   * A pipeline can specify a complete (or partial) recipe for end-to-end data modeling, including text pre-processing, hyper-parameter tuning, and cross-validation. A pipeline in sklearn is a tool that sequentially applies a list of transforms and a final estimator. Intermediate steps of the pipeline must implement fit and transform methods. The final estimator only needs to implement fit. The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. This makes the code cleaner and easier to manage, helps to avoid common mistakes like data leakage, and makes it easier to systematically compare the performance of different configurations.
* Why do we want to apply vectorization after we separate into training and testing sets?
   * So we don't provide the vectorizer information from the test set (i.e. data leakage). The vectorizer should be fitted on the training set, which means it learns the vocabulary of the training set. It's important that it doesn't have access to the test set because in a real-world scenario, the model will encounter unseen data after it's been trained. Therefore, the test set should mimic this situation and be completely unseen by the model during the training process.
* What is the best description of latent semantic analysis (LSA)?
   * When we apply singular value decomposition (SVD) to a term-document matrix. LSA is a technique in NLP for analyzing relationships between a set of documents and the terms they contain. It does this by producing a set of concepts related to the documents and terms. It uses SVD, which is a factorization method in linear algebra, to reduce the dimensionality of the term-document matrix and reveal the underlying semantic structure of the terms and documents.
* When we analyze a document using LSA what are we trying to find?
   * The various topics that appear in the document. LSA helps us find the underlying "topics" or "concepts" in a document. These topics are groups of words that tend to occur together in the documents. LSA helps us find an abstract representation of these topics.
* Why do we apply different vectorization methods in classification tasks?
   * Each method captures different information that is encoded in text data. Different vectorization techniques represent text data in different ways. For example, CountVectorizer transforms the text by representing it as a bag of words, and disregards structure but keeps a count of word occurrence. TfidfVectorizer, on the other hand, considers how important a word is to a document in a collection or corpus. The goal of using different vectorization techniques is to experiment with how different representations of the text data affect the model's learning and performance.
* When we create the first pipeline (before we apply LSA), which two components are we using in our Pipeline?
   * We are using a vectorizer and a classifier. In a typical NLP pipeline, the first component is usually a vectorizer, which transforms the raw text data into a numerical format that machine learning algorithms can understand. The second component is a classifier, a machine learning model that is trained on the transformed data to perform a task, such as classifying the text data into different categories.
* What type of dimensional reduction technique did we implement when we applied Latent Semantic Analysis (LSA) to our text data?
   * Singular value decomposition. LSA uses SVD, a method from linear algebra, to reduce the dimensionality of the term-document matrix. The goal is to identify the most important directions in the data, or where the data holds the most information, and disregard the rest. This helps to reduce the noise and redundancy in the data.
* Which of the following vectorizers could we use in a pipeline with Latent Semantic Analysis (LSA)?
   * A CountVectorizer() and TfidfVectorizer(). Both these vectorizers can be used with LSA. They both transform the text data into a format that can be processed by LSA. CountVectorizer encodes the text data as a bag of words, while TfidfVectorizer weights the word counts by how often they appear in the documents.
* How does spaCy's document vectorization differ from the CountVectorizer() or TfidfVectorizer() methods?
   * The spaCy nlp(doc).vector returns the vector which is the average of all of the word vectors in the document. spaCy's vectorization method differs from CountVectorizer and TfidfVectorizer in that it doesn't just look at word counts or term frequencies. Instead, spaCy uses pre-trained word vectors that capture the semantic meaning of words based on their context. When spaCy vectorizes a document, it averages the word vectors for each word in the document to produce a document vector.